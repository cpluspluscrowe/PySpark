{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@67adbb41\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6265e8b9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"Samples\").setMaster(\"local\")\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark Hive Example\")\n",
    "  .config(conf)\n",
    "  .getOrCreate()\n",
    "//val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to put together a simple linear regression model in spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.fpm.PrefixSpan\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.LinearRegression\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[36mpoints\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: int, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLinearRegression\u001b[39m = linReg_4fe15d67c0ed\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mregression\u001b[39m.\u001b[32mLinearRegressionModel\u001b[39m = linReg_4fe15d67c0ed\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataFrame\u001b[39m = [features: vector]\n",
       "\u001b[36mpredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [features: vector, prediction: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import org.apache.spark.mllib.fpm.PrefixSpan\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.sql.types._\n",
    "val points = spark.createDataFrame(Seq(\n",
    "  (1620000,Vectors.dense(2100)),\n",
    "  (1690000,Vectors.dense(2300)),\n",
    "  (1400000,Vectors.dense(2046)),\n",
    "  (2000000,Vectors.dense(4314)),\n",
    "  (1060000,Vectors.dense(1244)),\n",
    "  (3830000,Vectors.dense(4608)),\n",
    "  (1230000,Vectors.dense(2173)),\n",
    "  (2400000,Vectors.dense(2750)),\n",
    "  (3380000,Vectors.dense(4010)),\n",
    "  (1480000,Vectors.dense(1959))\n",
    "  )).toDF(\"label\",\"features\")\n",
    "val lr = new LinearRegression()\n",
    "val model = lr.fit(points)\n",
    "val test = spark.createDataFrame(Seq(Vectors.dense(2100)).map(Tuple1.apply)).toDF(\"features\")\n",
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "execution_count": 18,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "<console>:1: identifier expected but string literal found.",
      "println(test[\"features\"])",
      "             ^",
      "<console>:1: ']' expected but eof found.",
      "println(test[\"features\"])",
      "                         ^"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
