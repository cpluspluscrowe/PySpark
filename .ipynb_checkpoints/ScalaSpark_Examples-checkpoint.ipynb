{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@67adbb41\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6265e8b9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"Samples\").setMaster(\"local\")\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark Hive Example\")\n",
    "  .config(conf)\n",
    "  .getOrCreate()\n",
    "//val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to put together a simple linear regression model in spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.fpm.PrefixSpan\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.LinearRegression\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[36mpoints\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: int, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLinearRegression\u001b[39m = linReg_4fe15d67c0ed\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mregression\u001b[39m.\u001b[32mLinearRegressionModel\u001b[39m = linReg_4fe15d67c0ed\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataFrame\u001b[39m = [features: vector]\n",
       "\u001b[36mpredictions\u001b[39m: \u001b[32mDataFrame\u001b[39m = [features: vector, prediction: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import org.apache.spark.mllib.fpm.PrefixSpan\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.sql.types._\n",
    "val points = spark.createDataFrame(Seq(\n",
    "  (1620000,Vectors.dense(2100)),\n",
    "  (1690000,Vectors.dense(2300)),\n",
    "  (1400000,Vectors.dense(2046)),\n",
    "  (2000000,Vectors.dense(4314)),\n",
    "  (1060000,Vectors.dense(1244)),\n",
    "  (3830000,Vectors.dense(4608)),\n",
    "  (1230000,Vectors.dense(2173)),\n",
    "  (2400000,Vectors.dense(2750)),\n",
    "  (3380000,Vectors.dense(4010)),\n",
    "  (1480000,Vectors.dense(1959))\n",
    "  )).toDF(\"label\",\"features\")\n",
    "val lr = new LinearRegression()\n",
    "val model = lr.fit(points)\n",
    "val test = spark.createDataFrame(Seq(Vectors.dense(2100)).map(Tuple1.apply)).toDF(\"features\")\n",
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.LinearRegression\n",
       "\u001b[39m\n",
       "\u001b[36mpoints\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLinearRegression\u001b[39m = linReg_baa7d8a131b5\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mregression\u001b[39m.\u001b[32mLinearRegressionModel\u001b[39m = linReg_baa7d8a131b5\n",
       "\u001b[36mres23_6\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mVector\u001b[39m = [0.18531922804008738,0.013616539127918346,0.0,0.0,0.0,0.009917465418232699,0.0,0.0,0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "val points = spark.createDataFrame(Seq(\n",
    "    (1d,Vectors.dense(5,3,1,2,1,3,2,2,1)),\n",
    "    (2d,Vectors.dense(9,8,8,9,7,9,8,7,9))\n",
    ")).toDF(\"label\",\"features\")\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(.3).setFitIntercept(false).setElasticNetParam(1.0)\n",
    "val model = lr.fit(points)\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.Vectors\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.LinearRegression\n",
       "\u001b[39m\n",
       "\u001b[36mpoints\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLinearRegression\u001b[39m = linReg_90ecce706162\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mregression\u001b[39m.\u001b[32mLinearRegressionModel\u001b[39m = linReg_90ecce706162\n",
       "\u001b[36mres27_6\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mVector\u001b[39m = [0.11329331633450115,0.03937073300046667,0.002369276442275244,0.010416987598811298,0.0043289885742031475,0.026236646722551396,0.015282817648377314,0.023597219133656366,0.0011928984792447094]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "val points = spark.createDataFrame(Seq(\n",
    "    (1d,Vectors.dense(5,3,1,2,1,3,2,2,1)),\n",
    "    (2d,Vectors.dense(9,8,8,9,7,9,8,7,9))\n",
    ")).toDF(\"label\",\"features\")\n",
    "val lr = new LinearRegression().setMaxIter(10).setRegParam(.3).setFitIntercept(false).setElasticNetParam(0.0)\n",
    "val model = lr.fit(points)\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification using Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.695906432748538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[36mtrainingDataSet\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_6518f7e247ca\n",
       "\u001b[36mmodel\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionModel\u001b[39m = logreg_6518f7e247ca\n",
       "\u001b[36mtrainingSummary\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionTrainingSummary\u001b[39m = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@51a62460\n",
       "\u001b[36mbinarySummary\u001b[39m: \u001b[32mBinaryLogisticRegressionSummary\u001b[39m = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@51a62460"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "val trainingDataSet = spark.createDataFrame(Seq(\n",
    "        (0.0,Vectors.dense(0.245)),\n",
    "        (0.0,Vectors.dense(0.247)),\n",
    "        (1.0,Vectors.dense(0.285)),\n",
    "        (1.0,Vectors.dense(0.299)),\n",
    "        (1.0,Vectors.dense(0.327)),\n",
    "        (1.0,Vectors.dense(0.347)),\n",
    "        (0.0,Vectors.dense(0.356)),\n",
    "        (1.0,Vectors.dense(0.36)),\n",
    "        (0.0,Vectors.dense(0.363)),\n",
    "        (1.0,Vectors.dense(0.364)),\n",
    "        (0.0,Vectors.dense(0.398)),\n",
    "        (1.0,Vectors.dense(0.4)),\n",
    "        (0.0,Vectors.dense(0.409)),\n",
    "        (1.0,Vectors.dense(0.421)),\n",
    "        (0.0,Vectors.dense(0.432)),\n",
    "        (1.0,Vectors.dense(0.473)),\n",
    "        (1.0,Vectors.dense(0.509)),\n",
    "        (1.0,Vectors.dense(0.529)),\n",
    "        (0.0,Vectors.dense(0.561)),\n",
    "        (0.0,Vectors.dense(0.569)),\n",
    "        (1.0,Vectors.dense(0.594)),\n",
    "        (1.0,Vectors.dense(0.638)),\n",
    "        (1.0,Vectors.dense(0.656)),\n",
    "        (1.0,Vectors.dense(0.816)),\n",
    "        (1.0,Vectors.dense(0.853)),\n",
    "        (1.0,Vectors.dense(0.938)),\n",
    "        (1.0,Vectors.dense(1.036)),\n",
    "        (1.0,Vectors.dense(1.045)))).toDF(\"label\",\"features\")\n",
    "val lr = new LogisticRegression\n",
    "val model = lr.fit(trainingDataSet)\n",
    "val trainingSummary = model.summary\n",
    "val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "println(s\"areaUnderROC: ${binarySummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clearer Logistic Regression with predictions for new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n",
      "+-----+------------+--------------------+--------------------+----------+\n",
      "|label|    features|       rawPrediction|         probability|prediction|\n",
      "+-----+------------+--------------------+--------------------+----------+\n",
      "|  1.0|[90.0,270.0]|[-61.884758625375...|[1.32981373753969...|       1.0|\n",
      "|  0.0|[62.0,120.0]|[31.4607691059637...|[0.99999999999997...|       0.0|\n",
      "+-----+------------+--------------------+--------------------+----------+\n",
      "\n",
      "+------------+----------+\n",
      "|    features|prediction|\n",
      "+------------+----------+\n",
      "|[90.0,270.0]|       1.0|\n",
      "|[62.0,120.0]|       0.0|\n",
      "+------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.LogisticRegression\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Vector,Vectors}\n",
       "\u001b[39m\n",
       "\u001b[36mlebron\u001b[39m: (\u001b[32mDouble\u001b[39m, \u001b[32mVector\u001b[39m) = (\u001b[32m1.0\u001b[39m, [80.0,250.0])\n",
       "\u001b[36mtim\u001b[39m: (\u001b[32mDouble\u001b[39m, \u001b[32mVector\u001b[39m) = (\u001b[32m0.0\u001b[39m, [70.0,150.0])\n",
       "\u001b[36mbrittany\u001b[39m: (\u001b[32mDouble\u001b[39m, \u001b[32mVector\u001b[39m) = (\u001b[32m1.0\u001b[39m, [80.0,207.0])\n",
       "\u001b[36mstacey\u001b[39m: (\u001b[32mDouble\u001b[39m, \u001b[32mVector\u001b[39m) = (\u001b[32m0.0\u001b[39m, [65.0,120.0])\n",
       "\u001b[36mtraining\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector]\n",
       "\u001b[36mestimator\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_57b4be71b62e\n",
       "\u001b[36mtransformer\u001b[39m: \u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionModel\u001b[39m = logreg_57b4be71b62e\n",
       "\u001b[36mjohn\u001b[39m: \u001b[32mVector\u001b[39m = [90.0,270.0]\n",
       "\u001b[36mtom\u001b[39m: \u001b[32mVector\u001b[39m = [62.0,120.0]\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector]\n",
       "\u001b[36mresults\u001b[39m: \u001b[32mDataFrame\u001b[39m = [label: double, features: vector ... 3 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector,Vectors}\n",
    "val lebron = (1.0,Vectors.dense(80.0,250.0))\n",
    "val tim = (0.0, Vectors.dense(70.0,150.0))\n",
    "val brittany = (1.0,Vectors.dense(80.0,207.0))\n",
    "val stacey = (0.0, Vectors.dense(65.0,120.0))\n",
    "val training = spark.createDataFrame(Seq(lebron,tim,brittany,stacey)).toDF(\"label\",\"features\")\n",
    "val estimator = new LogisticRegression\n",
    "val transformer = estimator.fit(training)\n",
    "val john = Vectors.dense(90.0,270.0)\n",
    "val tom = Vectors.dense(62.0,120.0)\n",
    "val test = spark.createDataFrame(Seq((1.0,john),(0.0,tom))).toDF(\"label\",\"features\")\n",
    "val results = transformer.transform(test)\n",
    "results.printSchema\n",
    "results.show\n",
    "val predictions = results.select(\"features\",\"prediction\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
